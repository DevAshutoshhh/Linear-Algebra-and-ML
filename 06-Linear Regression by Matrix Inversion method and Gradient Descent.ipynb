{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a420309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate Synthetic Dataset\n",
    "def generate_dataset(num_samples, num_features, noise=0.1):\n",
    "    #np.random.seed(42)  # For reproducibility\n",
    "    X = np.random.randn(num_samples, num_features)  # Matrix of order num_samples x num_features\n",
    "    true_weights = np.random.randn(num_features)   \n",
    "    true_bias = np.random.randn()\n",
    "    y = X @ true_weights + true_bias + noise * np.random.randn(num_samples)\n",
    "    return X, y, true_weights, true_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b3ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Inversion Solution\n",
    "def matrix_inversion_solution(X, Y):\n",
    "    X_matrix = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    Y_matrix = Y.reshape(-1, 1)\n",
    "    \n",
    "    # Use the normal equation: theta = (X^T X)^{-1} X^T Y\n",
    "    X_transpose_X = np.dot(X_matrix.T, X_matrix)\n",
    "    X_transpose_X_inv = np.linalg.inv(X_transpose_X)\n",
    "    X_transpose_Y = np.dot(X_matrix.T, Y_matrix)\n",
    "    \n",
    "    theta = np.dot(X_transpose_X_inv, X_transpose_Y)\n",
    "    return theta  # Return weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02168d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gradient Descent Implementation\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize weight vector and bias\n",
    "    weights = np.zeros(num_features)\n",
    "    bias = 0.0\n",
    "    \n",
    "    # To store the loss at each iteration\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Compute predictions\n",
    "        y_pred = X @ weights + bias\n",
    "\n",
    "        \n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = np.mean((y_pred - y) ** 2)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        \n",
    "        # Compute gradients\n",
    "        dw = (2 / num_samples) * (X.T @ (y_pred - y))\n",
    "        db = (2 / num_samples) * np.sum(y_pred - y)\n",
    "        \n",
    "        # Update weights and bias\n",
    "        weights = weights - learning_rate * dw\n",
    "        bias = bias - learning_rate * db\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Compute final loss (Mean Squared Error)\n",
    "    y_pred = X @ weights + bias\n",
    "    loss = np.mean((y_pred - y) ** 2)\n",
    "\n",
    "    return weights, bias, loss, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e4a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "# Example with one feature\n",
    "\n",
    "num_samples = 10\n",
    "num_features = 1\n",
    "X, y, true_weights, true_bias = generate_dataset(num_samples, num_features)\n",
    "#print(\"True Weights:\", true_weights)\n",
    "#print(\"True Bias:\", true_bias)\n",
    "\n",
    "# Generated data\n",
    "print(\"X: \",X)\n",
    "print(\"y: \",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Inversion Solution\n",
    "start_time1 = time.time()\n",
    "w_vector = matrix_inversion_solution(X, y)\n",
    "matrix_inv_time = time.time() - start_time1\n",
    "    \n",
    "print(\"Time taken by matrix inversion solution: \",matrix_inv_time,\" seconds\")\n",
    "print(\"Matrix Inversion solution: \",w_vector)\n",
    "\n",
    "# Here the first component of w_vector is the bias and the second component is the weight\n",
    "\n",
    "print(\"\\nBias computed by matrix inversion solution: \", w_vector[0])\n",
    "print(\"Weight computed by matrix inversion solution: \", w_vector[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Gradient Descent\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "\n",
    "start_time2 = time.time()\n",
    "learned_weights, learned_bias, loss, loss_history = gradient_descent(X, y, learning_rate, num_iterations)\n",
    "\n",
    "gradient_descent_time = time.time() - start_time2\n",
    "print(\"\\nTime taken by gradient descent: \", gradient_descent_time,\" seconds\")\n",
    "    \n",
    "print(\"\\nLearned Weight:\", learned_weights)\n",
    "print(\"Learned Bias:\", learned_bias)\n",
    "\n",
    "\n",
    "# Compare true and learned weights and bias\n",
    "#print(\"\\nDifference between true and learned biases:\", true_bias - learned_bias)          # This is a scalar\n",
    "#print(\"\\nDifference between true and learned weights:\", true_weights - learned_weights)   # This is a vector\n",
    "\n",
    "\n",
    "print(\"Loss (Mean squared error) with the learned weight and learned bias:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over iterations\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_iterations + 1), loss_history, color='blue')\n",
    "plt.title(\"Loss Over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b29fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.plot(X, X @ w_vector[1] + w_vector[0,0], color='red', label='Matrix Inversion Solution')  \n",
    "plt.plot(X, X @ learned_weights + learned_bias, color='green', linestyle='--', label='Gradient Descent')\n",
    "plt.title('Matrix Inversion vs Gradient Descent Regression Lines')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose the learning rate is too high.\n",
    "\n",
    "\n",
    "# Perform Gradient Descent\n",
    "learning_rate = 2\n",
    "num_iterations = 100\n",
    "\n",
    "start_time2 = time.time()\n",
    "learned_weights, learned_bias, loss, loss_history = gradient_descent(X, y, learning_rate, num_iterations)\n",
    "\n",
    "gradient_descent_time = time.time() - start_time2\n",
    "print(\"\\nTime taken by gradient descent: \", gradient_descent_time,\" seconds\")\n",
    "    \n",
    "print(\"\\nLearned Weights:\", learned_weights)\n",
    "print(\"Learned Bias:\", learned_bias)\n",
    "\n",
    "\n",
    "# Compare true and learned weights and bias\n",
    "#print(\"\\nDifference between true and learned biases:\", true_bias - learned_bias)          \n",
    "#print(\"\\nDifference between true and learned weights:\", true_weights - learned_weights)   \n",
    "\n",
    "\n",
    "print(\"Loss (Mean squared error) with the learned weights and learned bias:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fe722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over iterations\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_iterations + 1), loss_history, color='blue')\n",
    "plt.title(\"Loss Over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df04564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE WITH MORE FEATURES\n",
    "\n",
    "\n",
    "# Generate data\n",
    "\n",
    "num_samples = 1000000\n",
    "num_features = 100\n",
    "X, y, true_weights, true_bias = generate_dataset(num_samples, num_features)\n",
    "#print(\"True Weights:\", true_weights)\n",
    "#print(\"True Bias:\", true_bias)\n",
    "\n",
    "# Generated data\n",
    "#print(\"X: \",X)\n",
    "#print(\"y: \",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Inversion Solution\n",
    "start_time1 = time.time()\n",
    "w_vector = matrix_inversion_solution(X, y)\n",
    "matrix_inv_time = time.time() - start_time1\n",
    "    \n",
    "print(\"Time taken by matrix inversion solution: \",matrix_inv_time,\" seconds\")\n",
    "print(\"Matrix Inversion solution: \",w_vector)\n",
    "\n",
    "# Here the first component of w_vector is the bias and the remaining components are the weights\n",
    "\n",
    "print(\"\\nBias computed by matrix inversion solution: \", w_vector[0])\n",
    "print(\"Weights computed by matrix inversion solution: \", w_vector[1:len(w_vector)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76724f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Gradient Descent\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "\n",
    "start_time2 = time.time()\n",
    "learned_weights, learned_bias, loss, loss_history = gradient_descent(X, y, learning_rate, num_iterations)\n",
    "\n",
    "gradient_descent_time = time.time() - start_time2\n",
    "print(\"\\nTime taken by gradient descent: \", gradient_descent_time,\" seconds\")\n",
    "    \n",
    "print(\"\\nLearned Weight:\", learned_weights)\n",
    "print(\"Learned Bias:\", learned_bias)\n",
    "\n",
    "\n",
    "# Compare true and learned weights and bias\n",
    "#print(\"\\nDifference between true and learned biases:\", true_bias - learned_bias)          \n",
    "#print(\"\\nDifference between true and learned weights:\", true_weights - learned_weights)   \n",
    "\n",
    "\n",
    "print(\"Loss (Mean squared error) with the learned weight and learned bias:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over iterations\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_iterations + 1), loss_history, color='blue')\n",
    "plt.title(\"Loss Over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d704504",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6189557f",
   "metadata": {},
   "source": [
    "EXERCISES:\n",
    "\n",
    "1. Demonstrate the effect of low learning rate on the convergence of gradient descent.\n",
    "2. Demonstrate the effect of high learning rate on the convergence of gradient descent.\n",
    "3. Demonstrate an example where the matrix inversion solution is slower than gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772870a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
